{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37924595",
   "metadata": {},
   "source": [
    "### 경차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30da142",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "HTTP 400 url=https://api.encar.com/search/car/list/general?count=true&q=%28And.Hidden.N._.C.CarType.Y._.Category.%EA%B2%BD%EC%B0%A8._.Condition.Inspection._.Condition.Record.%29&sr=%7CModifiedDate%7C0%7C1 body=",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    118\u001b[39m         crawl_all_from_action(action, sort=\u001b[33m\"\u001b[39m\u001b[33mModifiedDate\u001b[39m\u001b[33m\"\u001b[39m, limit=\u001b[32m50\u001b[39m, csv_path=CSV_PATH)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_dc_url:\n\u001b[32m    114\u001b[39m     dc_url = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://www.encar.com/dc/dc_carsearchlist.do#!\u001b[39m\u001b[33m%\u001b[39m\u001b[33m7B\u001b[39m\u001b[38;5;132;01m%22a\u001b[39;00m\u001b[33mction\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m3A\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22(And.Hidden.N._.C.CarType.Y._.Category.\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mA\u001b[39m\u001b[33m%\u001b[39m\u001b[33mB2\u001b[39m\u001b[33m%\u001b[39m\u001b[33mBD\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mC\u001b[39m\u001b[33m%\u001b[39m\u001b[33mB0\u001b[39m\u001b[33m%\u001b[39m\u001b[33mA8._.Condition.Inspection._.Condition.Record.)\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m2C\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22toggle\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m3A\u001b[39m\u001b[33m%\u001b[39m\u001b[33m7B\u001b[39m\u001b[33m%\u001b[39m\u001b[33m7D\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2C\u001b[39m\u001b[38;5;132;01m%22la\u001b[39;00m\u001b[33myer\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m3A\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m22\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2C\u001b[39m\u001b[38;5;132;01m%22s\u001b[39;00m\u001b[33mort\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m3A\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22ModifiedDate\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m2C\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22page\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m3A1\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2C\u001b[39m\u001b[38;5;132;01m%22li\u001b[39;00m\u001b[33mmit\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m3A20\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2C\u001b[39m\u001b[38;5;132;01m%22s\u001b[39;00m\u001b[33mearchKey\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m3A\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m22\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2C\u001b[39m\u001b[38;5;132;01m%22lo\u001b[39;00m\u001b[33mginCheck\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m3Afalse\u001b[39m\u001b[33m%\u001b[39m\u001b[33m7D\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[43mcrawl_all_from_dc_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    117\u001b[39m     action = build_action([\u001b[33m\"\u001b[39m\u001b[33m경차\u001b[39m\u001b[33m\"\u001b[39m], car_type=\u001b[33m\"\u001b[39m\u001b[33mY\u001b[39m\u001b[33m\"\u001b[39m, inspection=\u001b[38;5;28;01mTrue\u001b[39;00m, record=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mcrawl_all_from_dc_url\u001b[39m\u001b[34m(dc_url, limit, csv_path, sleep_sec)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcrawl_all_from_dc_url\u001b[39m(dc_url, limit=\u001b[32m50\u001b[39m, csv_path=CSV_PATH, sleep_sec=\u001b[32m0.6\u001b[39m):\n\u001b[32m    108\u001b[39m     action, sort = parse_dc_url(dc_url)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[43mcrawl_all_from_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msleep_sec\u001b[49m\u001b[43m=\u001b[49m\u001b[43msleep_sec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mcrawl_all_from_action\u001b[39m\u001b[34m(action, sort, limit, csv_path, sleep_sec)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcrawl_all_from_action\u001b[39m(action, sort=\u001b[33m\"\u001b[39m\u001b[33mModifiedDate\u001b[39m\u001b[33m\"\u001b[39m, limit=\u001b[32m50\u001b[39m, csv_path=CSV_PATH, sleep_sec=\u001b[32m0.6\u001b[39m):\n\u001b[32m     90\u001b[39m     s = make_session()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     total = \u001b[43mget_total_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m total == \u001b[32m0\u001b[39m:\n\u001b[32m     93\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo items found\u001b[39m\u001b[33m\"\u001b[39m); \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mget_total_count\u001b[39m\u001b[34m(s, action, sort)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_total_count\u001b[39m(s, action, sort):\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     j = \u001b[43mget_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mq\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msort\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m|0|1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(j.get(\u001b[33m\"\u001b[39m\u001b[33mCount\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mget_json\u001b[39m\u001b[34m(s, params, tag)\u001b[39m\n\u001b[32m     37\u001b[39m r = s.get(BASE_URL, params=params, timeout=\u001b[32m15\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m url=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m body=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.text[:\u001b[32m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m r.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).lower():\n\u001b[32m     41\u001b[39m     (DATA_DIR / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time.time())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.html\u001b[39m\u001b[33m\"\u001b[39m).write_text(r.text, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: HTTP 400 url=https://api.encar.com/search/car/list/general?count=true&q=%28And.Hidden.N._.C.CarType.Y._.Category.%EA%B2%BD%EC%B0%A8._.Condition.Inspection._.Condition.Record.%29&sr=%7CModifiedDate%7C0%7C1 body="
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, unquote\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "if '__file__' in globals():\n",
    "    REPO_ROOT = Path(__file__).resolve().parent.parent\n",
    "else:\n",
    "    REPO_ROOT = Path.cwd().parent\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "CSV_PATH = DATA_DIR / \"light_cars.csv\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://api.encar.com/search/car/list/general\"\n",
    "HEADERS = {\n",
    "    \"user-agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/140.0.0.0 Safari/537.36\"),\n",
    "    \"accept\": \"application/json, text/plain, */*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"origin\": \"https://www.encar.com\",\n",
    "    \"referer\": \"https://www.encar.com/dc/dc_carsearchlist.do\",\n",
    "}\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1.2,\n",
    "                    status_forcelist=[429, 500, 502, 503, 504],\n",
    "                    allowed_methods=[\"GET\"])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def get_json(s, params, tag):\n",
    "    r = s.get(BASE_URL, params=params, timeout=15)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code} url={r.url} body={r.text[:200]}\")\n",
    "    if \"application/json\" not in r.headers.get(\"Content-Type\",\"\").lower():\n",
    "        (DATA_DIR / f\"{tag}_{int(time.time())}.html\").write_text(r.text, encoding=\"utf-8\")\n",
    "        raise ValueError(\"Non-JSON\")\n",
    "    return r.json()\n",
    "\n",
    "def normalize_sort(x):\n",
    "    if not x: return \"ModifiedDate\"\n",
    "    t = str(x).strip()\n",
    "    if t.lower() == \"modifieddate\" or t == \"ModifiedDate\": return \"ModifiedDate\"\n",
    "    return \"ModifiedDate\"\n",
    "\n",
    "def parse_dc_url(url: str):\n",
    "    frag = urlparse(url).fragment\n",
    "    if not frag: raise ValueError(\"no #! fragment\")\n",
    "    decoded = unquote(frag)\n",
    "    if \"!\" in decoded:\n",
    "        decoded = decoded.split(\"!\")[-1]\n",
    "        decoded = unquote(decoded)\n",
    "    i, j = decoded.find(\"{\"), decoded.rfind(\"}\")\n",
    "    if i == -1 or j == -1: raise ValueError(\"no JSON in fragment\")\n",
    "    obj = json.loads(decoded[i:j+1])\n",
    "    obj = {k.lower(): v for k, v in obj.items()}\n",
    "    action = obj.get(\"action\")\n",
    "    sort = normalize_sort(obj.get(\"sort\"))\n",
    "    if not action: raise ValueError(\"action missing\")\n",
    "    return action, sort\n",
    "\n",
    "def build_action(categories, car_type=\"Y\", inspection=False, record=False):\n",
    "    names = [str(c).strip() for c in categories if c and str(c).strip()]\n",
    "    names = list(dict.fromkeys(names))\n",
    "    elems = [f\"C.CarType.{car_type}.\"]\n",
    "    if names:\n",
    "        if len(names) == 1:\n",
    "            elems.append(f\"Category.{names[0]}.\")\n",
    "            inner = \"._.\".join(elems)\n",
    "            if inspection: inner += \"._.Condition.Inspection.\"\n",
    "            if record: inner += \"._.Condition.Record.\"\n",
    "            return f\"(And.Hidden.N._.({inner}))\"\n",
    "        joined = \"Category.\" + \"._.Category.\".join(names) + \".\"\n",
    "        elems.append(f\"(Or.{joined})\")\n",
    "    inner = \"._.\".join(elems)\n",
    "    if inspection: inner += \"._.Condition.Inspection.\"\n",
    "    if record: inner += \"._.Condition.Record.\"\n",
    "    return f\"(And.Hidden.N._.({inner}))\"\n",
    "\n",
    "def get_total_count(s, action, sort):\n",
    "    j = get_json(s, {\"count\":\"true\",\"q\":action,\"sr\":f\"|{sort}|0|1\"}, \"count\")\n",
    "    return int(j.get(\"Count\", 0) or 0)\n",
    "\n",
    "def crawl_all_from_action(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH, sleep_sec=0.6):\n",
    "    s = make_session()\n",
    "    total = get_total_count(s, action, sort)\n",
    "    if total == 0:\n",
    "        print(\"No items found\"); return\n",
    "    if csv_path.exists(): csv_path.unlink()\n",
    "    wrote_header = False\n",
    "    for offset in range(0, total, limit):\n",
    "        params = {\"count\":\"false\",\"q\":action,\"sr\":f\"|{sort}|{offset}|{limit}\"}\n",
    "        data = get_json(s, params, \"page\")\n",
    "        rows = data.get(\"SearchResults\", [])\n",
    "        if not rows: break\n",
    "        df = pd.json_normalize(rows, max_level=1)\n",
    "        df.to_csv(csv_path, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=not wrote_header)\n",
    "        wrote_header = True\n",
    "        time.sleep(sleep_sec)\n",
    "    print(f\"완료: {csv_path}\")\n",
    "\n",
    "def crawl_all_from_dc_url(dc_url, limit=50, csv_path=CSV_PATH, sleep_sec=0.6):\n",
    "    action, sort = parse_dc_url(dc_url)\n",
    "    crawl_all_from_action(action, sort=sort, limit=limit, csv_path=csv_path, sleep_sec=sleep_sec)\n",
    "\n",
    "def main():\n",
    "    use_dc_url = True\n",
    "    if use_dc_url:\n",
    "        dc_url = r\"https://www.encar.com/dc/dc_carsearchlist.do#!%7B%22action%22%3A%22(And.Hidden.N._.C.CarType.Y._.Category.%EA%B2%BD%EC%B0%A8._.Condition.Inspection._.Condition.Record.)%22%2C%22toggle%22%3A%7B%7D%2C%22layer%22%3A%22%22%2C%22sort%22%3A%22ModifiedDate%22%2C%22page%22%3A1%2C%22limit%22%3A20%2C%22searchKey%22%3A%22%22%2C%22loginCheck%22%3Afalse%7D\"\n",
    "        crawl_all_from_dc_url(dc_url, limit=50, csv_path=CSV_PATH)\n",
    "    else:\n",
    "        action = build_action([\"경차\"], car_type=\"Y\", inspection=True, record=True)\n",
    "        crawl_all_from_action(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2d4598",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     99\u001b[39m     crawl_with_action(action, sort=sort, limit=\u001b[32m50\u001b[39m, csv_path=CSV_PATH)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     97\u001b[39m dc_url = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://www.encar.com/dc/dc_carsearchlist.do#!\u001b[39m\u001b[33m%\u001b[39m\u001b[33m7B\u001b[39m\u001b[38;5;132;01m%22a\u001b[39;00m\u001b[33mction\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m3A\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22(And.Hidden.N._.(C.CarType.Y._.(Or.Category.\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mA\u001b[39m\u001b[33m%\u001b[39m\u001b[33mB2\u001b[39m\u001b[33m%\u001b[39m\u001b[33mBD\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mC\u001b[39m\u001b[33m%\u001b[39m\u001b[33mB0\u001b[39m\u001b[33m%\u001b[39m\u001b[33mA8._.Category.\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mC\u001b[39m\u001b[38;5;132;01m%86%\u001b[39;00m\u001b[33m8C\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mD\u001b[39m\u001b[38;5;132;01m%98%\u001b[39;00m\u001b[33m95\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mC\u001b[39m\u001b[33m%\u001b[39m\u001b[33mB0\u001b[39m\u001b[33m%\u001b[39m\u001b[33mA8._.Category.\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mC\u001b[39m\u001b[33m%\u001b[39m\u001b[33mA4\u001b[39m\u001b[38;5;132;01m%80%\u001b[39;00m\u001b[33mEC\u001b[39m\u001b[33m%\u001b[39m\u001b[33mA4\u001b[39m\u001b[38;5;132;01m%91%\u001b[39;00m\u001b[33mED\u001b[39m\u001b[38;5;132;01m%98%\u001b[39;00m\u001b[33m95\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mC\u001b[39m\u001b[33m%\u001b[39m\u001b[33mB0\u001b[39m\u001b[33m%\u001b[39m\u001b[33mA8._.Category.\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mC\u001b[39m\u001b[33m%\u001b[39m\u001b[33mA4\u001b[39m\u001b[38;5;132;01m%91%\u001b[39;00m\u001b[33mED\u001b[39m\u001b[38;5;132;01m%98%\u001b[39;00m\u001b[33m95\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mC\u001b[39m\u001b[33m%\u001b[39m\u001b[33mB0\u001b[39m\u001b[33m%\u001b[39m\u001b[33mA8._.Category.\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mB\u001b[39m\u001b[33m%\u001b[39m\u001b[33m8C\u001b[39m\u001b[38;5;132;01m%80%\u001b[39;00m\u001b[33mED\u001b[39m\u001b[38;5;132;01m%98%\u001b[39;00m\u001b[33m95\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[33mC\u001b[39m\u001b[33m%\u001b[39m\u001b[33mB0\u001b[39m\u001b[33m%\u001b[39m\u001b[33mA8._.Category.SUV.)))\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22,\u001b[39m\u001b[38;5;132;01m%22s\u001b[39;00m\u001b[33mort\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22:\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22ModifiedDate\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22,\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22page\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22:1,\u001b[39m\u001b[38;5;132;01m%22li\u001b[39;00m\u001b[33mmit\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22:20,\u001b[39m\u001b[38;5;132;01m%22la\u001b[39;00m\u001b[33myer\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22:\u001b[39m\u001b[38;5;132;01m%22%\u001b[39;00m\u001b[33m22,\u001b[39m\u001b[38;5;132;01m%22lo\u001b[39;00m\u001b[33mginCheck\u001b[39m\u001b[33m%\u001b[39m\u001b[33m22:false\u001b[39m\u001b[33m%\u001b[39m\u001b[33m7D\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     98\u001b[39m action, sort, page, limit = parse_dc_url(dc_url)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[43mcrawl_with_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mcrawl_with_action\u001b[39m\u001b[34m(action, sort, limit, csv_path)\u001b[39m\n\u001b[32m     89\u001b[39m         time.sleep(\u001b[32m3\u001b[39m)\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     time.sleep(\u001b[32m0.6\u001b[39m)\n\u001b[32m     92\u001b[39m df = pd.json_normalize(all_items, max_level=\u001b[32m1\u001b[39m)\n\u001b[32m     93\u001b[39m df.to_csv(csv_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8-sig\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os, json, math, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, unquote\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "if '__file__' in globals():\n",
    "    REPO_ROOT = Path(__file__).resolve().parent.parent\n",
    "else:\n",
    "    REPO_ROOT = Path.cwd().parent\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "CSV_PATH = DATA_DIR / \"domestic_cars.csv\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://api.encar.com/search/car/list/premium\"\n",
    "HEADERS = {\n",
    "    \"user-agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/140.0.0.0 Safari/537.36\"),\n",
    "    \"accept\": \"application/json, text/plain, */*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"origin\": \"https://www.encar.com\",\n",
    "    \"referer\": \"https://www.encar.com/dc/dc_carsearchlist.do\",\n",
    "}\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1.2,\n",
    "                    status_forcelist=[429, 500, 502, 503, 504],\n",
    "                    allowed_methods=[\"GET\"])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def get_json(s: requests.Session, params: dict, error_prefix: str):\n",
    "    r = s.get(BASE_URL, params=params, timeout=15)\n",
    "    ct = r.headers.get(\"Content-Type\", \"\")\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code} url={r.url} body={r.text[:200]}\")\n",
    "    if \"application/json\" not in ct.lower():\n",
    "        fname = DATA_DIR / f\"{error_prefix}_{int(time.time())}.html\"\n",
    "        fname.write_text(r.text, encoding=\"utf-8\")\n",
    "        raise ValueError(f\"Non-JSON response saved to {fname}\")\n",
    "    return r.json()\n",
    "\n",
    "def parse_dc_url(url: str):\n",
    "    frag = urlparse(url).fragment\n",
    "    if not frag:\n",
    "        raise ValueError(\"No hash fragment found in URL.\")\n",
    "    decoded = unquote(frag)\n",
    "    if \"!%7B\" in frag or decoded.count(\"{\") == 0:\n",
    "        parts = decoded.split(\"!\")\n",
    "        decoded = unquote(parts[-1])\n",
    "    if not decoded.strip().startswith(\"{\"):\n",
    "        i, j = decoded.find(\"{\"), decoded.rfind(\"}\")\n",
    "        if i == -1 or j == -1:\n",
    "            raise ValueError(\"No JSON found in fragment.\")\n",
    "        decoded = decoded[i:j+1]\n",
    "    obj = json.loads(decoded)\n",
    "    obj = {k.lower(): v for k, v in obj.items()}\n",
    "    action = obj.get(\"action\")\n",
    "    sort = obj.get(\"sort\") or \"ModifiedDate\"\n",
    "    page = int(obj.get(\"page\") or 1)\n",
    "    limit = int(obj.get(\"limit\") or 20)\n",
    "    if not action:\n",
    "        raise ValueError(\"action not found in fragment JSON.\")\n",
    "    return action, sort, page, limit\n",
    "\n",
    "def crawl_with_action(action: str, sort: str = \"ModifiedDate\", limit: int = 50, csv_path: Path = CSV_PATH):\n",
    "    s = make_session()\n",
    "    base_params = {\"count\": \"false\", \"q\": action}\n",
    "    first = get_json(s, {**base_params, \"sr\": f\"|{sort}|0|1\"}, \"encar_first_error\")\n",
    "    total = int(first.get(\"Count\", 0) or 0)\n",
    "    if total == 0:\n",
    "        print(\"No items found.\")\n",
    "        return\n",
    "    items_per_page = limit\n",
    "    pages = math.ceil(total / items_per_page)\n",
    "    all_items = []\n",
    "    for p in range(pages):\n",
    "        offset = p * items_per_page\n",
    "        params = {**base_params, \"sr\": f\"|{sort}|{offset}|{items_per_page}\"}\n",
    "        try:\n",
    "            data = get_json(s, params, \"encar_page_error\")\n",
    "            all_items.extend(data.get(\"SearchResults\", []))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] page={p+1} offset={offset} error={e}\")\n",
    "            time.sleep(3)\n",
    "            continue\n",
    "        time.sleep(0.6)\n",
    "    df = pd.json_normalize(all_items, max_level=1)\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"총 {len(df)}개 저장 -> {csv_path}\")\n",
    "\n",
    "def main():\n",
    "    dc_url = r\"https://www.encar.com/dc/dc_carsearchlist.do#!%7B%22action%22%3A%22(And.Hidden.N._.(C.CarType.Y._.(Or.Category.%EA%B2%BD%EC%B0%A8._.Category.%EC%86%8C%ED%98%95%EC%B0%A8._.Category.%EC%A4%80%EC%A4%91%ED%98%95%EC%B0%A8._.Category.%EC%A4%91%ED%98%95%EC%B0%A8._.Category.%EB%8C%80%ED%98%95%EC%B0%A8._.Category.SUV.)))%22,%22sort%22:%22ModifiedDate%22,%22page%22:1,%22limit%22:20,%22layer%22:%22%22,%22loginCheck%22:false%7D\"\n",
    "    action, sort, page, limit = parse_dc_url(dc_url)\n",
    "    crawl_with_action(action, sort=sort, limit=50, csv_path=CSV_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6176cc52",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "HTTP 400 url=https://api.encar.com/search/car/list/premium?count=false&q=%28And.Hidden.N._.%28C.CarType.Y._.%28Or.Category.SUV.._.Category.%EA%B2%BD%EC%B0%A8.._.Category.%EB%8C%80%ED%98%95%EC%B0%A8.._.Category.%EC%86%8C%ED%98%95%EC%B0%A8.._.Category.%EC%A4%91%ED%98%95%EC%B0%A8.%29%29%29&sr=%7CModifiedDate%7C0%7C20 body=",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     94\u001b[39m     crawl_one_page(action, sort=sort, page=page, limit=limit, csv_path=CSV_PATH)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     92\u001b[39m page = \u001b[32m1\u001b[39m\n\u001b[32m     93\u001b[39m limit = \u001b[32m20\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mcrawl_one_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mcrawl_one_page\u001b[39m\u001b[34m(action, sort, page, limit, csv_path)\u001b[39m\n\u001b[32m     80\u001b[39m offset = \u001b[38;5;28mmax\u001b[39m(page - \u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m) * limit\n\u001b[32m     81\u001b[39m params = {\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mq\u001b[39m\u001b[33m\"\u001b[39m: action, \u001b[33m\"\u001b[39m\u001b[33msr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msort\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m data = \u001b[43mget_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencar_page_error\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m rows = data.get(\u001b[33m\"\u001b[39m\u001b[33mSearchResults\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m     84\u001b[39m df = pd.json_normalize(rows, max_level=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mget_json\u001b[39m\u001b[34m(s, params, error_prefix)\u001b[39m\n\u001b[32m     38\u001b[39m ct = r.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m url=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m body=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.text[:\u001b[32m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ct.lower():\n\u001b[32m     42\u001b[39m     fname = DATA_DIR / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time.time())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.html\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: HTTP 400 url=https://api.encar.com/search/car/list/premium?count=false&q=%28And.Hidden.N._.%28C.CarType.Y._.%28Or.Category.SUV.._.Category.%EA%B2%BD%EC%B0%A8.._.Category.%EB%8C%80%ED%98%95%EC%B0%A8.._.Category.%EC%86%8C%ED%98%95%EC%B0%A8.._.Category.%EC%A4%91%ED%98%95%EC%B0%A8.%29%29%29&sr=%7CModifiedDate%7C0%7C20 body="
     ]
    }
   ],
   "source": [
    "import os, json, math, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, unquote\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "if '__file__' in globals():\n",
    "    REPO_ROOT = Path(__file__).resolve().parent.parent\n",
    "else:\n",
    "    REPO_ROOT = Path.cwd().parent\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "CSV_PATH = DATA_DIR / \"domestic_cars.csv\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://api.encar.com/search/car/list/premium\"\n",
    "HEADERS = {\n",
    "    \"user-agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/140.0.0.0 Safari/537.36\"),\n",
    "    \"accept\": \"application/json, text/plain, */*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"origin\": \"https://www.encar.com\",\n",
    "    \"referer\": \"https://www.encar.com/dc/dc_carsearchlist.do\",\n",
    "}\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1.2,\n",
    "                    status_forcelist=[429, 500, 502, 503, 504],\n",
    "                    allowed_methods=[\"GET\"])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def get_json(s: requests.Session, params: dict, error_prefix: str):\n",
    "    r = s.get(BASE_URL, params=params, timeout=15)\n",
    "    ct = r.headers.get(\"Content-Type\", \"\")\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code} url={r.url} body={r.text[:200]}\")\n",
    "    if \"application/json\" not in ct.lower():\n",
    "        fname = DATA_DIR / f\"{error_prefix}_{int(time.time())}.html\"\n",
    "        fname.write_text(r.text, encoding=\"utf-8\")\n",
    "        raise ValueError(f\"Non-JSON response saved to {fname}\")\n",
    "    return r.json()\n",
    "\n",
    "def parse_dc_url(url: str):\n",
    "    frag = urlparse(url).fragment\n",
    "    if not frag:\n",
    "        raise ValueError(\"No hash fragment found in URL.\")\n",
    "    decoded = unquote(frag)\n",
    "    if \"!%7B\" in frag or decoded.count(\"{\") == 0:\n",
    "        parts = decoded.split(\"!\")\n",
    "        decoded = unquote(parts[-1])\n",
    "    if not decoded.strip().startswith(\"{\"):\n",
    "        i, j = decoded.find(\"{\"), decoded.rfind(\"}\")\n",
    "        if i == -1 or j == -1:\n",
    "            raise ValueError(\"No JSON found in fragment.\")\n",
    "        decoded = decoded[i:j+1]\n",
    "    obj = json.loads(decoded)\n",
    "    obj = {k.lower(): v for k, v in obj.items()}\n",
    "    action = obj.get(\"action\")\n",
    "    sort = obj.get(\"sort\") or \"ModifiedDate\"\n",
    "    page = int(obj.get(\"page\") or 1)\n",
    "    limit = int(obj.get(\"limit\") or 20)\n",
    "    if not action:\n",
    "        raise ValueError(\"action not found in fragment JSON.\")\n",
    "    return action, sort, page, limit\n",
    "\n",
    "def build_action_from_categories(categories, car_type=\"Y\"):\n",
    "    cats = [str(c).strip() for c in categories if c and str(c).strip()]\n",
    "    cats = sorted(set(cats))\n",
    "    if not cats:\n",
    "        return f\"(And.Hidden.N._.(C.CarType.{car_type}.))\"\n",
    "    inner = \"._.\".join([f\"Category.{c}.\" for c in cats])\n",
    "    return f\"(And.Hidden.N._.(C.CarType.{car_type}._.(Or.{inner})))\"\n",
    "\n",
    "def crawl_one_page(action: str, sort: str = \"ModifiedDate\", page: int = 1, limit: int = 20, csv_path: Path = CSV_PATH):\n",
    "    s = make_session()\n",
    "    offset = max(page - 1, 0) * limit\n",
    "    params = {\"count\": \"false\", \"q\": action, \"sr\": f\"|{sort}|{offset}|{limit}\"}\n",
    "    data = get_json(s, params, \"encar_page_error\")\n",
    "    rows = data.get(\"SearchResults\", [])\n",
    "    df = pd.json_normalize(rows, max_level=1)\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"총 {len(df)}개 저장 -> {csv_path}\")\n",
    "\n",
    "def main():\n",
    "    categories = [\"경차\", \"소형차\", \"중형차\", \"대형차\", \"SUV\"]\n",
    "    action = build_action_from_categories(categories, car_type=\"Y\")\n",
    "    sort = \"ModifiedDate\"\n",
    "    page = 1\n",
    "    limit = 20\n",
    "    crawl_one_page(action, sort=sort, page=page, limit=limit, csv_path=CSV_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efa5a36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1155개 저장 -> c:\\Users\\User\\Desktop\\Project\\backend\\data-pipeline\\data\\compact_cars.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "if '__file__' in globals():\n",
    "    REPO_ROOT = Path(__file__).resolve().parent.parent\n",
    "else:\n",
    "    REPO_ROOT = Path.cwd().parent\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "CSV_PATH = DATA_DIR / \"compact_cars.csv\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://api.encar.com/search/car/list/premium\"\n",
    "HEADERS = {\n",
    "    \"user-agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/140.0.0.0 Safari/537.36\"),\n",
    "    \"accept\": \"application/json, text/plain, */*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"origin\": \"https://www.encar.com\",\n",
    "    \"referer\": \"https://www.encar.com/dc/dc_carsearchlist.do\",\n",
    "}\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1.2,\n",
    "                    status_forcelist=[429, 500, 502, 503, 504],\n",
    "                    allowed_methods=[\"GET\"])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def get_json(s, params, tag):\n",
    "    r = s.get(BASE_URL, params=params, timeout=15)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code} url={r.url} body={r.text[:200]}\")\n",
    "    if \"application/json\" not in r.headers.get(\"Content-Type\",\"\").lower():\n",
    "        (DATA_DIR / f\"{tag}_{int(time.time())}.html\").write_text(r.text, encoding=\"utf-8\")\n",
    "        raise ValueError(\"Non-JSON\")\n",
    "    return r.json()\n",
    "\n",
    "def build_action_from_categories(categories, car_type=\"Y\"):\n",
    "    names = [str(c).strip() for c in categories if c and str(c).strip()]\n",
    "    names = list(dict.fromkeys(names))\n",
    "    if not names:\n",
    "        return f\"(And.Hidden.N._.(C.CarType.{car_type}.))\"\n",
    "    joined = \"Category.\" + \"._.Category.\".join(names) + \".\"\n",
    "    return f\"(And.Hidden.N._.(C.CarType.{car_type}._.(Or.{joined})))\"\n",
    "\n",
    "def get_total_count(s, action, sort):\n",
    "    j = get_json(s, {\"count\":\"true\", \"q\":action, \"sr\":f\"|{sort}|0|1\"}, \"count\")\n",
    "    return int(j.get(\"Count\", 0) or 0)\n",
    "\n",
    "def crawl_all_pages(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH, sleep_sec=0.6):\n",
    "    s = make_session()\n",
    "    total = get_total_count(s, action, sort)\n",
    "    if total == 0:\n",
    "        print(\"No items found\"); return\n",
    "    if csv_path.exists():\n",
    "        csv_path.unlink()\n",
    "    saved = 0\n",
    "    wrote_header = False\n",
    "    for offset in range(0, total, limit):\n",
    "        params = {\"count\":\"false\", \"q\":action, \"sr\":f\"|{sort}|{offset}|{limit}\"}\n",
    "        data = get_json(s, params, \"encar_page_error\")\n",
    "        rows = data.get(\"SearchResults\", [])\n",
    "        if not rows:\n",
    "            break\n",
    "        df = pd.json_normalize(rows, max_level=1)\n",
    "        df.to_csv(csv_path, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=not wrote_header)\n",
    "        wrote_header = True\n",
    "        saved += len(df)\n",
    "        time.sleep(sleep_sec)\n",
    "    print(f\"총 {saved}개 저장 -> {csv_path}\")\n",
    "\n",
    "def main():\n",
    "    categories = [\"소형차\"]\n",
    "    action = build_action_from_categories(categories, car_type=\"Y\")\n",
    "    crawl_all_pages(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ffb266",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     81\u001b[39m     crawl_all_pages(action, sort=\u001b[33m\"\u001b[39m\u001b[33mModifiedDate\u001b[39m\u001b[33m\"\u001b[39m, limit=\u001b[32m50\u001b[39m, csv_path=CSV_PATH)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     79\u001b[39m categories = [\u001b[33m\"\u001b[39m\u001b[33m소형차\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     80\u001b[39m action = build_action_from_categories(categories, car_type=\u001b[33m\"\u001b[39m\u001b[33mY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mcrawl_all_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mModifiedDate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mcrawl_all_pages\u001b[39m\u001b[34m(action, sort, limit, csv_path, sleep_sec)\u001b[39m\n\u001b[32m     73\u001b[39m     wrote_header = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     74\u001b[39m     saved += \u001b[38;5;28mlen\u001b[39m(df)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     time.sleep(sleep_sec)\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m총 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msaved\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m개 저장 -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "if '__file__' in globals():\n",
    "    REPO_ROOT = Path(__file__).resolve().parent.parent\n",
    "else:\n",
    "    REPO_ROOT = Path.cwd().parent\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "CSV_PATH = DATA_DIR / \"compact_cars.csv\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://api.encar.com/search/car/list/premium\"\n",
    "HEADERS = {\n",
    "    \"user-agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/140.0.0.0 Safari/537.36\"),\n",
    "    \"accept\": \"application/json, text/plain, */*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"origin\": \"https://www.encar.com\",\n",
    "    \"referer\": \"https://www.encar.com/dc/dc_carsearchlist.do\",\n",
    "}\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1.2,\n",
    "                    status_forcelist=[429, 500, 502, 503, 504],\n",
    "                    allowed_methods=[\"GET\"])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def get_json(s, params, tag):\n",
    "    r = s.get(BASE_URL, params=params, timeout=15)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code} url={r.url} body={r.text[:200]}\")\n",
    "    if \"application/json\" not in r.headers.get(\"Content-Type\",\"\").lower():\n",
    "        (DATA_DIR / f\"{tag}_{int(time.time())}.html\").write_text(r.text, encoding=\"utf-8\")\n",
    "        raise ValueError(\"Non-JSON\")\n",
    "    return r.json()\n",
    "\n",
    "def build_action_from_categories(categories, car_type=\"Y\"):\n",
    "    names = [str(c).strip() for c in categories if c and str(c).strip()]\n",
    "    names = list(dict.fromkeys(names))\n",
    "    if not names:\n",
    "        return f\"(And.Hidden.N._.(C.CarType.{car_type}._.Condition.Record._.Condition.Inspection.))\"\n",
    "    joined = \"Category.\" + \"._.Category.\".join(names) + \"._.Condition.Record._.Condition.Inspection.\"\n",
    "    return f\"(And.Hidden.N._.(C.CarType.{car_type}._.(Or.{joined})))\"\n",
    "\n",
    "def get_total_count(s, action, sort):\n",
    "    j = get_json(s, {\"count\":\"true\", \"q\":action, \"sr\":f\"|{sort}|0|1\"}, \"count\")\n",
    "    return int(j.get(\"Count\", 0) or 0)\n",
    "\n",
    "def crawl_all_pages(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH, sleep_sec=0.6):\n",
    "    s = make_session()\n",
    "    total = get_total_count(s, action, sort)\n",
    "    if total == 0:\n",
    "        print(\"No items found\"); return\n",
    "    if csv_path.exists():\n",
    "        csv_path.unlink()\n",
    "    saved = 0\n",
    "    wrote_header = False\n",
    "    for offset in range(0, total, limit):\n",
    "        params = {\"count\":\"false\", \"q\":action, \"sr\":f\"|{sort}|{offset}|{limit}\"}\n",
    "        data = get_json(s, params, \"encar_page_error\")\n",
    "        rows = data.get(\"SearchResults\", [])\n",
    "        if not rows:\n",
    "            break\n",
    "        df = pd.json_normalize(rows, max_level=1)\n",
    "        df.to_csv(csv_path, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=not wrote_header)\n",
    "        wrote_header = True\n",
    "        saved += len(df)\n",
    "        time.sleep(sleep_sec)\n",
    "    print(f\"총 {saved}개 저장 -> {csv_path}\")\n",
    "\n",
    "def main():\n",
    "    categories = [\"소형차\"]\n",
    "    action = build_action_from_categories(categories, car_type=\"Y\")\n",
    "    crawl_all_pages(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b10366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1153개 저장 -> c:\\Users\\User\\Desktop\\Project\\backend\\data-pipeline\\data\\compact_cars.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "if '__file__' in globals():\n",
    "    REPO_ROOT = Path(__file__).resolve().parent.parent\n",
    "else:\n",
    "    REPO_ROOT = Path.cwd().parent\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "CSV_PATH = DATA_DIR / \"compact_cars_kor.csv\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://api.encar.com/search/car/list/premium\"\n",
    "HEADERS = {\n",
    "    \"user-agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/140.0.0.0 Safari/537.36\"),\n",
    "    \"accept\": \"application/json, text/plain, */*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"origin\": \"https://www.encar.com\",\n",
    "    \"referer\": \"https://www.encar.com/dc/dc_carsearchlist.do\",\n",
    "}\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1.2,\n",
    "                    status_forcelist=[429, 500, 502, 503, 504],\n",
    "                    allowed_methods=[\"GET\"])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def get_json(s, params, tag):\n",
    "    r = s.get(BASE_URL, params=params, timeout=15)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code} url={r.url} body={r.text[:200]}\")\n",
    "    if \"application/json\" not in r.headers.get(\"Content-Type\",\"\").lower():\n",
    "        (DATA_DIR / f\"{tag}_{int(time.time())}.html\").write_text(r.text, encoding=\"utf-8\")\n",
    "        raise ValueError(\"Non-JSON\")\n",
    "    return r.json()\n",
    "\n",
    "def build_action_from_categories(categories, car_type=\"Y\"):\n",
    "    names = [str(c).strip() for c in categories if c and str(c).strip()]\n",
    "    names = list(dict.fromkeys(names))\n",
    "    if not names:\n",
    "        return f\"(And.Hidden.N._.(C.CarType.{car_type}.))\"\n",
    "    joined = \"Category.\" + \"._.Category.\".join(names) + \".\"\n",
    "    return f\"(And.Hidden.N._.(C.CarType.{car_type}._.(Or.{joined})))\"\n",
    "\n",
    "def get_total_count(s, action, sort):\n",
    "    j = get_json(s, {\"count\":\"true\", \"q\":action, \"sr\":f\"|{sort}|0|1\"}, \"count\")\n",
    "    return int(j.get(\"Count\", 0) or 0)\n",
    "\n",
    "def make_detail_url(cid: str) -> str:\n",
    "    return f\"https://fem.encar.com/cars/detail/{cid}?pageid=dc_carsearch&listAdvType=pic&carid={cid}&view_type=normal\"\n",
    "\n",
    "def crawl_all_pages(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH, sleep_sec=0.6):\n",
    "    s = make_session()\n",
    "    total = get_total_count(s, action, sort)\n",
    "    if total == 0:\n",
    "        print(\"No items found\"); return\n",
    "    if csv_path.exists():\n",
    "        csv_path.unlink()\n",
    "    saved = 0\n",
    "    wrote_header = False\n",
    "    for offset in range(0, total, limit):\n",
    "        params = {\"count\":\"false\", \"q\":action, \"sr\":f\"|{sort}|{offset}|{limit}\"}\n",
    "        data = get_json(s, params, \"encar_page_error\")\n",
    "        rows = data.get(\"SearchResults\", [])\n",
    "        if not rows:\n",
    "            break\n",
    "        df = pd.json_normalize(rows, max_level=1)\n",
    "        id_col = next((c for c in [\"Id\",\"id\",\"carId\",\"carid\"] if c in df.columns), None)\n",
    "        if id_col:\n",
    "            df[\"detail_url\"] = df[id_col].astype(str).map(make_detail_url)\n",
    "        df.to_csv(csv_path, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=not wrote_header)\n",
    "        wrote_header = True\n",
    "        saved += len(df)\n",
    "        time.sleep(sleep_sec)\n",
    "    print(f\"총 {saved}개 저장 -> {csv_path}\")\n",
    "\n",
    "def main():\n",
    "    categories = [\"소형차\"]\n",
    "    action = build_action_from_categories(categories, car_type=\"Y\")\n",
    "    crawl_all_pages(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "if '__file__' in globals():\n",
    "    REPO_ROOT = Path(__file__).resolve().parent.parent\n",
    "else:\n",
    "    REPO_ROOT = Path.cwd().parent\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "CSV_PATH = DATA_DIR / \"small_cars_kor.csv\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://api.encar.com/search/car/list/premium\"\n",
    "HEADERS = {\n",
    "    \"user-agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/140.0.0.0 Safari/537.36\"),\n",
    "    \"accept\": \"application/json, text/plain, */*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"origin\": \"https://www.encar.com\",\n",
    "    \"referer\": \"https://www.encar.com/dc/dc_carsearchlist.do\",\n",
    "}\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1.2,\n",
    "                    status_forcelist=[429, 500, 502, 503, 504],\n",
    "                    allowed_methods=[\"GET\"])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def get_json(s, params, tag):\n",
    "    r = s.get(BASE_URL, params=params, timeout=15)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code} url={r.url} body={r.text[:200]}\")\n",
    "    if \"application/json\" not in r.headers.get(\"Content-Type\",\"\").lower():\n",
    "        (DATA_DIR / f\"{tag}_{int(time.time())}.html\").write_text(r.text, encoding=\"utf-8\")\n",
    "        raise ValueError(\"Non-JSON\")\n",
    "    return r.json()\n",
    "\n",
    "def build_action_from_categories(categories, car_type=\"Y\"):\n",
    "    names = [str(c).strip() for c in categories if c and str(c).strip()]\n",
    "    names = list(dict.fromkeys(names))\n",
    "    if not names:\n",
    "        return f\"(And.Hidden.N._.(C.CarType.{car_type}.))\"\n",
    "    joined = \"Category.\" + \"._.Category.\".join(names) + \".\"\n",
    "    return f\"(And.Hidden.N._.(C.CarType.{car_type}._.(Or.{joined})))\"\n",
    "\n",
    "def get_total_count(s, action, sort):\n",
    "    j = get_json(s, {\"count\":\"true\", \"q\":action, \"sr\":f\"|{sort}|0|1\"}, \"count\")\n",
    "    return int(j.get(\"Count\", 0) or 0)\n",
    "\n",
    "def make_detail_url(cid: str) -> str:\n",
    "    return f\"https://fem.encar.com/cars/detail/{cid}?pageid=dc_carsearch&listAdvType=pic&carid={cid}&view_type=normal\"\n",
    "\n",
    "def crawl_all_pages(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH, sleep_sec=0.6):\n",
    "    s = make_session()\n",
    "    total = get_total_count(s, action, sort)\n",
    "    if total == 0:\n",
    "        print(\"No items found\"); return\n",
    "    if csv_path.exists():\n",
    "        csv_path.unlink()\n",
    "    saved = 0\n",
    "    wrote_header = False\n",
    "    for offset in range(0, total, limit):\n",
    "        params = {\"count\":\"false\", \"q\":action, \"sr\":f\"|{sort}|{offset}|{limit}\"}\n",
    "        data = get_json(s, params, \"encar_page_error\")\n",
    "        rows = data.get(\"SearchResults\", [])\n",
    "        if not rows:\n",
    "            break\n",
    "        df = pd.json_normalize(rows, max_level=1)\n",
    "        id_col = next((c for c in [\"Id\",\"id\",\"carId\",\"carid\"] if c in df.columns), None)\n",
    "        if id_col:\n",
    "            df[\"detail_url\"] = df[id_col].astype(str).map(make_detail_url)\n",
    "        df.to_csv(csv_path, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=not wrote_header)\n",
    "        wrote_header = True\n",
    "        saved += len(df)\n",
    "        time.sleep(sleep_sec)\n",
    "    print(f\"총 {saved}개 저장 -> {csv_path}\")\n",
    "\n",
    "def main():\n",
    "    categories = [\"경차\"]\n",
    "    action = build_action_from_categories(categories, car_type=\"Y\")\n",
    "    crawl_all_pages(action, sort=\"ModifiedDate\", limit=50, csv_path=CSV_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carfin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
